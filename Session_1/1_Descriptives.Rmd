---
title: "Text Mining Basics"
subtitle: "Summer School JGU Mainz — Advanced Methods in Behavioral Economics, 2021"
author: "Carina I. Hausladen"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
bibliography: ss21.bib  
---


```{r setup, include=FALSE, cache = TRUE}
rm(list = ls())
library(quanteda)
library(quanteda.textstats)
library(devtools)
library(quantedaData)
library(ggplot2)
library(readtext)
library(nsyllable)
devtools::install_github("hadley/emo")
devtools::install_github("kbenoit/quantedaData")
options(warn = -1)
```

## Methods you will encounter 
In this first module, we will investigate the basics of text mining. You will learn about the following concepts:

* What are corpus, types, tokens, sentences?
* What is a readability score?
* Many text mining/NLP methods need a numeric representation of the text. The starting point for that is a document frequency matrix. 
* You will also learn about how to (pairwise) compare texts. We will look at one score, namely cosine similarity. 
* A simple way to analyze whether specific information is available in a text is the keywords in context analysis. 

## R and quanteda
* In this tutorial, we will mostly use one package which is called quanteda. 
* quanteda is an R package for managing and analyzing textual data.
* The package is designed for applying natural language processing to texts.
* It is heavily used by political and social scientists.


# Context: Political Speech and Divisiveness
"Most citizens want a secure country, safe neighborhoods and good schools. 
These issues do get discussed, of course, but a disproportionate amount of attention goes to issues like abortion, medical marijuana, and other narrow issues that simply do not motivate the great majority." @Fiorina2005, p. 202).

## Context: Political Speech and Divisiveness

- @Ash2017 use floor speeches by Members of the U.S. Congress and find that U.S. Senators spend more time on divisive issues when they are up for election. 
- In this tutorial, we investigate divisiveness of political speech via Plenarprotokolle from the German Bundestag. 

## Data
To increase computational speed, the data consists only of 5 protocols from the [German Bundestag](https://www.bundestag.de/services/opendata).
I downloaded the .xml files and transformed them to using a Python script to .txt (`xml_to_txt.py`) beforehand. 

```{r rd_df, include=TRUE, cache=TRUE}
temp = list.files(path="data/prtkll/19WP",pattern="*.txt")
df = do.call(rbind, lapply(temp, function(x) readtext(paste("data/prtkll/19WP/",x,sep=""))))
```

`r emo::ji("nerd")`: There are additional protocol files in the `data/prtkll/19WP/unused` folder. You can adapt the data directory to try out different data sets.


## Types, tokens, sentences
[Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/types-tokens/)

* From "Sacred Emily", by Gertrude Stein:
**Rose is a rose is a rose is a rose.**
* Includes three different types.
* Includes ten different tokens.

```{r tkns, include=TRUE, cache=TRUE}
corpus.both<-corpus(df)
s<-summary(corpus.both)
```

On average, each document consists of `r round(mean(s$Types), digits=2)` types, `r round(mean(s$Tokens), digits=2)` tokens, and `r round(mean(s$Sentences), digits=2)` sentences.

## Readability Scores

Readability is the ease with which a reader can understand a written text.
We will look at one particular score, namely the Flesch's Reading Ease (FRE) Score [@Flesch1948].

### Characteristics
- Uses a combination of syllables and sentence length to indicate the complexity of speech
- Optimized for English
- The higher the FRE value, the easier the text

### Usage

* It can be interesting to calculate the readability of newspaper articles, books, Wikipedia articles. 
* Grammarly uses it also to provide recommendations for the writer.
* An interesting political science application is delivered by @Spirling2016. 
The authors investigate the impact of the Second Reform Act on the linguistic complexity of speeches made by members of parliament in Great Britain.

---

### Flesch's Reading Ease Score

$$
N_{sy} = \text{number of syllables} \\
N_w = \text{number of words} \\
N_s = \text{number of sentences} \\
\text{Average Sentence Length (ASL)} = \frac{N_w}{N_s} \\
206.835−(1.015× ASL)−(84.6×\frac{N_{sy}}{N_w}) 
$$
```{r rdblty, echo=TRUE, cache=TRUE}
readability<-textstat_readability(corpus.both, "Flesch")
```
The lowest readability score is `r round(min(readability$Flesch), digits = 2)`, the highest is `r round(max(readability$Flesch), digits = 2)`.

---
 
A high readability score is driven by a high number of sentences (which implies short sentences) as well as by a high number of syllables per word.
```{r rdblty_plt, echo=FALSE, fig.width=3, fig.height=3,fig.show='hold',fig.align='center', cache = TRUE}
syllable<-nsyllable(tokens(corpus.both)) #function needs a token object
syllables<-c() #create empty vector
for(i in 1:length(syllable)){
  su<-sum(unlist(syllable[i]), na.rm = TRUE)
  syllables[i]<-su
}

plot(syllables,readability$Flesch, 
     xlab = "Syllables",
     ylab = "Readability")
plot(s$Sentences,readability$Flesch, 
     xlab = "Sentences",
     ylab = "Readability")
```

## Document Frequency Matrix

* A document-feature matrix (dfm) shows the documents in the rows, the features in the columns.
* It can be interpreted as the implementation of the bag-of-words concept (more about that later).
* It often builds the basis for further NLP methods, such as classification.
* The dfm, sorted in a decreasing (increasing) way, shows the most (least) common tokens.
* The sparsity of a dfm denotes the proportion of $0$ in the matrix. Dfms can become quite large and sparse.

---

```{r dfm, echo=TRUE, cache = TRUE}
both.dfm<-dfm(tokens(corpus.both), 
              remove = stopwords("german"),
              remove_punct= TRUE)
head(dfm_sort(both.dfm, decreasing = TRUE))
```

## Cosine Similarity
- The cosine similarity $k$ is the cosine of the angle between vectors.
- It quantifies the similarity between two or more vectors.
- The more similar the frequencies, the smaller the angle between the vectors.
- $k$ close to $1$ indicates high similarity.
- If $x$ and $y$ are row vectors, their cosine similarity $k$ is defined as:
$$
k(x,y))=\frac{xy^T}{||x|| ||y||}
$$

---

### Cosine Similarity Applied
* A reference document is selected by chance.
* Usually, we are interested in document similarities for a corpus of documents. 
* Therefore, scaling methods (which we will encounter in module 3) are preferred over pairwise similarity measures like cosine similarity.
```{r csne, include=TRUE, cache = TRUE}
textstat_simil(both.dfm, method = "cosine", margin = "documents")
```

# Dictionary, summary statistics and keywords in context

## Dictionary to capture divisiveness
The dictionary is based on @Ash2017[ appendix, p.22].
It includes 19 words that denote divisive issues as well as 19 words and bigrams that denote common-values issues.
```{r dctnry, include=TRUE, cache=TRUE}
mydict <- dictionary(list(divisive = c("Stammzell*","Haushaltsgleichgewicht*","?lf*","Verm?genssteuer*",
"Krankenversicherung*","Stammzellenforschung*","Staatsverschuldung*",
"Bruttoinlandsprodukt*","Einkommenssteuer*","Steuererh?hung*",
"Drogenkontrolle*","fossile Brennstoffe","Abtreibung*","
Kleinunternehmer*","Inflation*","verschreibungspflichtig*","Sozialversicherung*",
"Mindestlohn*","Atomkrieg*"),
common_value = c("Finanzen*","Chemiewaffen*","?ffentlicher Dienst*","Staatsdiener*","Verteidigung*","
Menschenrecht*","Demokratie*","Verkehrsministeri*","st?dtische Finanz*","Staatshaushalt*","innere Sicherheit",
"medizinische Versorgung*","internationale Finanzverwaltung*",
"Strafverfolgung*","Gesetzesvollzug*","nationale Sicherheit*","privat* Sektor*",
"erneuerbare Energi*","Forschung und Entwicklung*")))
```

## Summary Statistics
```{r dfm_divisive, include=TRUE, cache=TRUE}
head(textstat_frequency(dfm(corpus.both, stem=TRUE, select = mydict$divisive)))
```

```{r dfm_common_value, include=TRUE, cache=TRUE}
head(textstat_frequency(dfm(corpus.both, stem =TRUE, select = mydict$common_value)))
```

## Category matches per document
The matrix shows that there are more matches for divisive words than for common-value words.
```{r ctgry_mtchs, echo=TRUE, cache=TRUE}
corpus.both.dfm<-dfm(corpus.both, dictionary = mydict) #category matches per text
corpus.both.dfm
```

## Log-ratio and document length
`r emo::ji("nerd")`: Find out whether logratio of divisive to common-value terms is independent of the document length (measured by tokens).
```{r lgrtio, echo=TRUE, cache=TRUE}
logratio<-c()
for(i in 1:corpus.both.dfm@Dim[1]){
 logratio[i]<-log(as.vector(corpus.both.dfm[i,2])/ as.vector(corpus.both.dfm[i,1]))
}

s$Tokens
```

## Key words in context

- Let's have a look at keywords in context. 
- We choose the three common-value tokens with the highest number of matches as keywords.
- We print the context as bigrams.
```{r kwic_print, echo=FALSE, cache=TRUE, warning=FALSE}
keywords<-c("demokrati*", "menschenrecht*", "finanzen*")

for(i in 1:length(keywords)){
print(keywords[i])
df<-kwic(corpus.both, keywords[i], window = 5)
dfb<-cbind(df$pre, df$post)
tokensAll <- tokens(char_tolower(dfb), remove_punct = TRUE)
tokensNoStopwords<-tokens_remove(tokensAll, stopwords("german"))
tokensNgramsNoStopwords <- tokens_ngrams(tokensNoStopwords, 2)
print(tail(sort(featnames(dfm(tokensNgramsNoStopwords, verbose = FALSE)))))
}
```

## References

# It's your turn! `r emo::ji("technologist")` ~10' 