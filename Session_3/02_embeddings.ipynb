{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "## Summer School JGU Mainz — Advanced Methods in Behavioral Economics, 2021\n",
    "\n",
    "### Carina I. Hausladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-25\n"
     ]
    }
   ],
   "source": [
    "print(now.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "* Tom Lin, 2019: [Blog](https://towardsdatascience.com/nlp-performance-of-different-word-embeddings-on-text-classification-de648c6262b), [github](https://github.com/TomLin/Playground/blob/master/04-Model-Comparison-Word2vec-Doc2vec-TfIdfWeighted.ipynb)\n",
    "* [pretrained embeddings](https://deepset.ai/german-word-embeddings)\n",
    "    * The deepset files need to be manually reformatted, according to the [gensim documentation](https://radimrehurek.com/gensim/scripts/glove2word2vec.html).\n",
    "    * **Please make sure that you have downloaded the pretrained embeddings (.magnitude files) according to the README.md and placed them in the data folder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- In the following, we will learn about various embedding methods.\n",
    "- We will use logistic regression in order to compare classification performance across embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from pymagnitude import Magnitude\n",
    "except ModuleNotFoundError:  # workaround for weird bug\n",
    "    from pymagnitude import Magnitude\n",
    "\n",
    "from utils.UtilWordEmbedding import DocModel, MeanEmbeddingVectorizer, TfidfEmbeddingVectorizer\n",
    "from utils.setup import prepare_docs\n",
    "from utils.strt_grp_sffl_splt import str_grp_splt\n",
    "from utils.utility import run_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chat_hours_simulated.csv')\n",
    "df_prep, all_docs = prepare_docs(df, y=\"honestmean\", X=\"Chat_subject\", dv=\"player.hours_stated\")\n",
    "all_docs.new_docs = [x if len(x) != 0 else \"kein_chat\" for x in all_docs.new_docs]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"honestmean\"].value_counts()  # 1: honest responses\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.802 | Pr: 0.687 | Re: 0.987 | AUC: 0.557 | Accuracy: 0.679 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(input='content', lowercase=False, preprocessor=lambda x: x)\n",
    "X_bow = bow.fit_transform(all_docs.new_docs)\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_bow[train_idx]\n",
    "test_X = X_bow[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "train_y.value_counts()\n",
    "test_y.value_counts()\n",
    "\n",
    "m_bow, model, ma = run_log_reg(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Tf–idf Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.797 | Pr: 0.673 | Re: 1.000 | AUC: 0.561 | Accuracy: 0.664 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content', lowercase=False, preprocessor=lambda x: x)\n",
    "X_tfidf = tfidf.fit_transform(all_docs.new_docs)\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_tfidf[train_idx]\n",
    "test_X = X_tfidf[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_tfidf = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Word2Vec\n",
    "[Mikolov et al. 2013](https://arxiv.org/abs/1301.3781), [Vatsal](https://towardsdatascience.com/word2vec-explained-49c52b4ccb71) <br>\n",
    "\n",
    "* Intuition\n",
    "    * Word2Vec groups vectors of similar words. \n",
    "    * It estimates a word's meaning based on its occurrences in the text. \n",
    "    * These estimates yield word associations with other words in the corpus.\n",
    "    * For example: \n",
    "    ```\n",
    "    King    -    Man    +    Woman    =    Queen\n",
    "    [5,3]   -    [2,1]  +    [3, 2]   =    [5,4]  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* How it works\n",
    "    * Words are transformed into a numerical representation of the word.\n",
    "    * This vector is learned via a neural network. \n",
    "    * The vectors try to capture various characteristics of that word, e.g. semantic relationship, context, etc. \n",
    "\n",
    "        ```\n",
    "        this = [1, 0, 0, 0, 0, 0, ... 0]\n",
    "        is    = [0, 1, 0, 0, 0, 0, ... 0]\n",
    "        fun = [0, 0, 1, 0, 0, 0, ... 0]\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Implementation\n",
    "    * We implement Word2Vec via gensim's [Word2vec class](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "    * Variations\n",
    "        * We can either train our own embeddings or use pretrained embeddings.\n",
    "        * Both types can be implemented via bow or tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## own, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.805 | Pr: 0.685 | Re: 1.000 | AUC: 0.525 | Accuracy: 0.681 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_own = Word2Vec(all_docs.doc_words, vector_size=70, min_count=1)\n",
    "mean_vec_tr = MeanEmbeddingVectorizer(w2v_own)  # averages all word embeddings occuring in the model\n",
    "doc_vec = mean_vec_tr.transform(all_docs.doc_words)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = doc_vec[train_idx]\n",
    "test_X = doc_vec[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_w2v_own_smpl = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## own, tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n",
      "WARNING:root:cannot compute average owing to no vector for []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.795 | Pr: 0.670 | Re: 1.000 | AUC: 0.458 | Accuracy: 0.660 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec_tr = TfidfEmbeddingVectorizer(w2v_own)\n",
    "tfidf_vec_tr.fit(all_docs.doc_words)\n",
    "tfidf_doc_vec = tfidf_vec_tr.transform(all_docs.doc_words)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = tfidf_doc_vec[train_idx]\n",
    "test_X = tfidf_doc_vec[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_w2v_own_tfidf = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## pre, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:19<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.803 | Pr: 0.685 | Re: 0.992 | AUC: 0.531 | Accuracy: 0.679 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python -m pymagnitude.converter -i 'analysis/data/vectors_w2v.txt' -o 'analysis/data/w2v.magnitude'\n",
    "w2v = Magnitude('data/w2v_vec.magnitude')\n",
    "\n",
    "def avg_embdngs(documents, embedings, num_trials=10):\n",
    "    vectors = []\n",
    "    for title in tqdm(documents):\n",
    "        try:\n",
    "            emb = np.average(embedings.query(word_tokenize(title)), axis=0)\n",
    "            vectors.append(emb)\n",
    "        except:\n",
    "            print(f\"Failed\")\n",
    "            print(title)\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "all_docs.new_docs = [x if len(x) != 0 else \"kein_chat\" for x in all_docs.new_docs]\n",
    "X_w2v_pre = avg_embdngs(all_docs.new_docs, w2v)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_w2v_pre[train_idx]\n",
    "test_X = X_w2v_pre[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_w2v_pre_smpl = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## pre, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:00<00:00, 2013.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.808 | Pr: 0.691 | Re: 0.995 | AUC: 0.529 | Accuracy: 0.688 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content', lowercase=False, preprocessor=lambda x: x)\n",
    "tfidf.fit(all_docs.new_docs)\n",
    "idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "\n",
    "\n",
    "def tfidf_embdngs(documents, embedings):\n",
    "    vectors = []\n",
    "    for title in tqdm(documents):\n",
    "        w2v_vectors = embedings.query(word_tokenize(title))\n",
    "        weights = [idf_dict.get(word, 1) for word in word_tokenize(title)]\n",
    "        vectors.append(np.average(w2v_vectors, axis=0, weights=weights))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "X_tfidf_w2v_pre = tfidf_embdngs(all_docs.new_docs, w2v)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_tfidf_w2v_pre[train_idx]\n",
    "test_X = X_tfidf_w2v_pre[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_w2v_pre_tfidf = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Global Vectors (GloVe)\n",
    "[nlp Stanford](https://nlp.stanford.edu/projects/glove/),\n",
    "[Pennington et al. (2014)](https://aclanthology.org/D14-1162.pdf),\n",
    "[Thushan Ganegedara](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010) <br>\n",
    "\n",
    "- GloVe is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "- Training is performed on aggregated global word-word co-occurrence statistics.\n",
    "- The resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Example\n",
    "    - Co-occurrence probabilities the for target words ice and steam.\n",
    "    - For the ratio, noise from non-discriminative words like water and fashion cancel out.\n",
    "    - Large values (> 1) correlate well with properties specific to ice.\n",
    "    - Small values (< 1) correlate well with properties specific of steam.\n",
    "\n",
    "| Probability and Ratio | k = solid | k = gas | k = water | k = fashion |\n",
    "| --------------------- | --------- | ------- | --------- | ----------- | \n",
    "| P(k\\|ice)| 1.9 × 10−4 | 6.6 × 10−5 | 3.0 × 10−3 | 1.7 × 10−5 |\n",
    "| P(k\\|steam)| 2.2 × 10−5 | 7.8 × 10−4 | 2.2 × 10−3 | 1.8 × 10−5 |\n",
    "| P(k\\|ice)/P(k\\|steam)| 8.9 | 8.5 × 10−2 | 1.36 | 0.96 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Word2Vec vs. GloVe\n",
    "    - Word2Vec \n",
    "        - Relies only on local information of language. \n",
    "        - Local: the semantics learnt for a given word is only affected by the surrounding words.\n",
    "    - GloVe\n",
    "       - Incorporates global and local statistics (word co-occurrence) to obtain word vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## glove, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:12<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.803 | Pr: 0.685 | Re: 0.994 | AUC: 0.520 | Accuracy: 0.679 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove = Magnitude('data/glove_vec.magnitude')\n",
    "X_glove_pre = avg_embdngs(all_docs.new_docs, glove)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_glove_pre[train_idx]\n",
    "test_X = X_glove_pre[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_glove_pre_smpl = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## pre, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 351/351 [00:00<00:00, 2099.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.802 | Pr: 0.684 | Re: 0.994 | AUC: 0.554 | Accuracy: 0.677 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content', lowercase=False, preprocessor=lambda x: x)\n",
    "tfidf.fit(all_docs.new_docs)\n",
    "idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "X_tfidf_glove_pre = tfidf_embdngs(all_docs.new_docs, glove)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_tfidf_glove_pre[train_idx]\n",
    "test_X = X_tfidf_glove_pre[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_glove_pre_tfidf = run_log_reg(train_X, test_X, train_y, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# fastText\n",
    "[Joulin et al. (2016)](https://arxiv.org/abs/1607.01759),\n",
    "[fasttext](https://fasttext.cc),\n",
    "[Nishan Subedi](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)<br>\n",
    "\n",
    "* Strengths:\n",
    "    * FastText achieves high performance for word representations and sentence classification.\n",
    "    * It performes specially strong in the case of rare words by making use of character level information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Bag of character n-grams\n",
    "\n",
    "    * Each word is represented as a bag of character n-grams in addition to the word itself. \n",
    "     * For example: word $matter$; $n = 3$; fastText representation:  `<ma, mat, att, tte, ter, er>`. \n",
    "     * `<` and `>` are added as boundary symbols to distinguish the ngram of a word from a word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* How it works\n",
    "    * The model is a bag of words model. \n",
    "    * Aside of the sliding window of n-gram selection, there is no internal structure of a word that is taken into account.\n",
    "    * As long as the characters fall under the window, the order of the character n-grams does not matter. \n",
    "    * During the model update, fastText learns weights for each of the n-grams as well as the entire word token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Load pretrained embeddings\n",
    "- We implemented fastText using logistic regression and sentence embeddings.\n",
    "- The fasttext API requires C++ Build Tools (Windows) or xtools (macOS).\n",
    "- These are quite big to install. Therefore, we load pretrained embeddings that were generated in another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_ft_own = np.load(\"data/fasttext_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Doc2Vec\n",
    "[Le and Mikolov (2014)](http://proceedings.mlr.press/v32/le14.html), \n",
    "[Edward Ma](https://towardsdatascience.com/understand-how-to-transfer-your-paragraph-to-vector-by-doc2vec-1e225ccf102) <br>\n",
    "\n",
    "* Intuition\n",
    "    * Instead of averaging word embeddings over the sequence, we can train paragraph vectors directly. \n",
    "    * We use document embeddings and distributed memory (PV-DM) instead of distributed bag of words. \n",
    "    * Randomly samples adjacent words from a paragraph. \n",
    "    * Predicts a center word from the sampled set by taking the context words and a paragraph id as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* How it works\n",
    "    * Doc2Vec is based on Word2Vec: It learns the document representation in an unsupervised manner.\n",
    "    * Input and output \n",
    "        * The input of texts (i.e. word) per document can vary.\n",
    "        * The output are vectors of fixed-length.\n",
    "    * Paragraph and word vectors\n",
    "        * The paragraph vector is unique among a document. \n",
    "        * Word vectors are shared among all documents.\n",
    "    * Training: Word vectors will be trained while the paragraph will be thrown away after that. \n",
    "    * Prediction: The paragraph vector will be initialized randomly and computed by word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## PV-DM\n",
    "Distributed Memory Model of Paragraph Vectors (PV-DM)\n",
    "* Both paragraph vectors and word vectors are initialized randomly. \n",
    "* Every paragraph vector is assigned to a single document while word vectors are shared among all documents. \n",
    "* Either averaging or concatenating both paragraph and words vectors.\n",
    "* Both are passed to stochastic gradient descent.\n",
    "* The gradient is obtained via back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.808 | Pr: 0.695 | Re: 0.989 | AUC: 0.532 | Accuracy: 0.690 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "workers = multiprocessing.cpu_count()\n",
    "dm_args = {'dm': 1, 'dm_mean': 1, 'vector_size': 100, 'window': 5, 'negative': 5, 'hs': 0, 'min_count': 2,\n",
    "           'sample': 0, 'workers': workers, 'alpha': 0.025, 'min_alpha': 0.025, 'epochs': 100,\n",
    "           'comment': 'alpha=0.025'\n",
    "           }\n",
    "dm = DocModel(docs=all_docs.tagdocs, **dm_args)\n",
    "dm.custom_train()\n",
    "dm_doc_vec_ls = []\n",
    "for i in range(len(dm.model.dv)):\n",
    "    dm_doc_vec_ls.append(dm.model.dv[i])\n",
    "dm_doc_vec = pd.DataFrame(dm_doc_vec_ls)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_prep,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = dm_doc_vec.loc[train_idx]\n",
    "test_X = dm_doc_vec.loc[test_idx]\n",
    "train_y = df_prep[\"honestmean\"][train_idx]\n",
    "test_y = df_prep[\"honestmean\"][test_idx]\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "m_dm_doc_vec = run_log_reg(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Plot Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>pr</th>\n",
       "      <th>re</th>\n",
       "      <th>AUC</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dm_doc_vec</th>\n",
       "      <td>0.808347</td>\n",
       "      <td>0.695347</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.531874</td>\n",
       "      <td>0.690426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v (pre, tfidf)</th>\n",
       "      <td>0.808110</td>\n",
       "      <td>0.691282</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>0.529391</td>\n",
       "      <td>0.688298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v (own, smpl)</th>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525294</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove (pre, smpl)</th>\n",
       "      <td>0.803108</td>\n",
       "      <td>0.684908</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>0.519867</td>\n",
       "      <td>0.678723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v (pre, smpl)</th>\n",
       "      <td>0.802874</td>\n",
       "      <td>0.685397</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.531234</td>\n",
       "      <td>0.678723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bow</th>\n",
       "      <td>0.802120</td>\n",
       "      <td>0.686807</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>0.556580</td>\n",
       "      <td>0.678723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove (pre, tfidf)</th>\n",
       "      <td>0.802088</td>\n",
       "      <td>0.683522</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>0.553815</td>\n",
       "      <td>0.676596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <td>0.796923</td>\n",
       "      <td>0.673095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.560778</td>\n",
       "      <td>0.663830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v (own, tfidf)</th>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.670213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457655</td>\n",
       "      <td>0.659574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft (own, smpl)</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.675676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          f1        pr        re       AUC       acc\n",
       "dm_doc_vec          0.808347  0.695347  0.988889  0.531874  0.690426\n",
       "w2v (pre, tfidf)    0.808110  0.691282  0.995238  0.529391  0.688298\n",
       "w2v (own, smpl)     0.805195  0.684783  1.000000  0.525294  0.680851\n",
       "glove (pre, smpl)   0.803108  0.684908  0.993651  0.519867  0.678723\n",
       "w2v (pre, smpl)     0.802874  0.685397  0.992063  0.531234  0.678723\n",
       "bow                 0.802120  0.686807  0.987302  0.556580  0.678723\n",
       "glove (pre, tfidf)  0.802088  0.683522  0.993651  0.553815  0.676596\n",
       "tfidf               0.796923  0.673095  1.000000  0.560778  0.663830\n",
       "w2v (own, tfidf)    0.794872  0.670213  1.000000  0.457655  0.659574\n",
       "ft (own, smpl)      0.769231  0.650794  0.976190  0.678571  0.675676"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [list(m_bow), list(m_tfidf[0]),\n",
    "       list(m_w2v_own_smpl[0]), list(m_w2v_own_tfidf[0]),\n",
    "       list(m_w2v_pre_smpl[0]), list(m_w2v_pre_tfidf[0]),\n",
    "       list(m_glove_pre_smpl[0]), list(m_glove_pre_tfidf[0]),\n",
    "       list(m_ft_own), list(m_dm_doc_vec[0])]\n",
    "\n",
    "df_results = pd.DataFrame(lst, columns=['f1', 'pr', 're', 'AUC', 'acc'], dtype=float)\n",
    "df_results.rename(index={0: 'bow', 1: 'tfidf',\n",
    "                         2: 'w2v (own, smpl)', 3: 'w2v (own, tfidf)',\n",
    "                         4: 'w2v (pre, smpl)', 5: 'w2v (pre, tfidf)',\n",
    "                         6: 'glove (pre, smpl)', 7: 'glove (pre, tfidf)',\n",
    "                         8: 'ft (own, smpl)', 9: 'dm_doc_vec'}, inplace=True)\n",
    "df_results = df_results.sort_values(by='f1', ascending=False)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}