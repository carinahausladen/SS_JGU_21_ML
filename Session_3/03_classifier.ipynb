{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Classifier\n",
    "\n",
    "## Summer School JGU Mainz â€” Advanced Methods in Behavioral Economics, 2021\n",
    "\n",
    "### Carina I. Hausladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-28\n"
     ]
    }
   ],
   "source": [
    "print(now.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Large parts of the following code are based on \n",
    "[Anirudh Shenoy](https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2)'s tutorial.  \n",
    "Pretrained embeddings are obtained from [deepset](https://deepset.ai/german-word-embeddings).\n",
    "\n",
    "- In this notebook, we will learn about different algorithms that can be used for classification tasks.\n",
    "- We will encounter stand-alone classifiers and ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "try:\n",
    "    from pymagnitude import Magnitude\n",
    "except ModuleNotFoundError:  # workaround for weird bug\n",
    "    from pymagnitude import Magnitude\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from utils.setup import prepare_docs\n",
    "from utils.strt_grp_sffl_splt import str_grp_splt\n",
    "from utils.utility import print_model_metrics, run_grid_search, fit_n_times, tfdf_embdngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chat_hours_simulated.csv')  \n",
    "df_new, all_docs = prepare_docs(df, y=\"honestmean\", X=\"Chat_subject\", dv=\"player.hours_stated\")\n",
    "all_docs.new_docs = [x if len(x) != 0 else \"kein_chat\" for x in all_docs.new_docs]\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Embeddings \n",
    "- To compare the performance of various classifiers, we will work with one type of embeddings.\n",
    "- We will use pre-trained, tf-idf weighted Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [00:25<00:00, 13.97it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='content', lowercase=False, preprocessor=lambda x: x)\n",
    "w2v = Magnitude('data/w2v_vec.magnitude')\n",
    "tfidf.fit(all_docs.new_docs)\n",
    "idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "X_tfidf_w2v_pre = tfdf_embdngs(all_docs.new_docs, w2v, dict_tf=idf_dict)\n",
    "\n",
    "train_idx, test_idx = str_grp_splt(df_new,\n",
    "                                   grp_col_name=\"group\",\n",
    "                                   y_col_name=\"honestmean\",\n",
    "                                   train_share=0.8)\n",
    "train_X = X_tfidf_w2v_pre[train_idx]\n",
    "test_X = X_tfidf_w2v_pre[test_idx]\n",
    "train_y = df_new[\"honestmean\"][train_idx]\n",
    "test_y = df_new[\"honestmean\"][test_idx]\n",
    "\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Cross-validation\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n",
    "\n",
    "- Learning the parameters of a prediction function and testing it on the same data is a methodological mistake.\n",
    "- The model would have a perfect score but it would fail to predict on unseen data (overfitting!). \n",
    "- To avoid it, it is common practice to hold out part of the available data as a test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- When evaluating hyperparameters for estimators, we risk overfitting on the test set.\n",
    "- To solve this problem, we partition the data into three sets:\n",
    "    - Training proceeds on the training set.\n",
    "    - Evaluation is done on the validation set.\n",
    "    - Final evaluation can be done on the test set.\n",
    "\n",
    "- Problem: \n",
    "    - We drastically reduce the number of samples which can be used for learning the model. \n",
    "    - The results can depend on a particular random choice for the splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Solution: cross-validation \n",
    "    - A test set is held out for final evaluation.\n",
    "    - k-fold CV: the training set is split into `k` smaller sets.\n",
    "    - If `k=5`, the dataset will be divided into 5 equal parts and the below process will run 5 times, each time with a different holdout set.\n",
    "\n",
    "```\n",
    "                    TRAINING DATA\n",
    "iteration 1 | TEST  | train | train | train | train\n",
    "iteration 2 | train | TEST  | train | train | train\n",
    "iteration 3 | train | train | TEST  | train | train\n",
    "iteration 4 | train | train | train | TEST  | train\n",
    "iteration 5 | train | train | train | train | TEST\n",
    "                                                    TEST DATA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Tuning Hyperparameters\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/grid_search.html#grid-search), [Rahil Shaikh](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85)\n",
    "\n",
    "* Hyperparameters are not directly learnt within estimators. \n",
    "* Typical examples include `C`, `kernel` and `gamma`, etc.\n",
    "* It is recommended to search the hyperparameter space for the best cross-validation score.\n",
    "\n",
    "We deploy an exhaustive Grid Search via [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). It searches over a specified parameter grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "def run_grid_search(model, params, x_train, y_train):\n",
    "    grid = GridSearchCV(model, params, cv=5, n_jobs=-1, \n",
    "                        scoring=score, verbose=0, refit=False)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return (grid.best_params_, grid.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Logistic Regression\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/sgd.html#sgd)<br>\n",
    "\n",
    "* Stochastic Gradient Descent (SGD) is an approach to fitting linear classifiers under convex loss functions such as Logistic Regression. \n",
    "* SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. \n",
    "* SGD is an optimization technique, in other words, it is a way to train a model. \n",
    "* `SGDClassifier(loss='log')` results in logistic regression.\n",
    "\n",
    "ðŸ¤“: Understand the elements of `lr_params`. Vary the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'alpha': 0.0001, 'l1_ratio': 0.5, 'penalty': 'elasticnet'}\n",
      "Best F1 : 0.6802349936143041\n",
      "F1: 0.808 | Pr: 0.689 | Re: 0.998 | AUC: 0.530 | Accuracy: 0.686 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log')\n",
    "lr_params = {'alpha': [10 ** (-x) for x in range(7)],\n",
    "             'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "             'l1_ratio': [0.15, 0.25, 0.5, 0.75]}\n",
    "best_params, best_f1 = run_grid_search(lr, lr_params, train_X, train_y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "print('Best F1 : {}'.format(best_f1))\n",
    "\n",
    "lr = SGDClassifier(loss='log',\n",
    "                   alpha=best_params['alpha'],\n",
    "                   penalty=best_params['penalty'],\n",
    "                   l1_ratio=best_params['l1_ratio'])\n",
    "metrics_lr = fit_n_times(lr, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## SVM\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/svm.html#classification)<br>\n",
    "We already know how SVMs work :)!  \n",
    "ðŸ¤“: Understand the elements of `svm_params`. Vary the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 100, 'degree': 2, 'kernel': 'linear'}\n",
      "Best F1 : 0.6767073722297603\n",
      "F1: 0.795 | Pr: 0.670 | Re: 1.000 | AUC: 0.502 | Accuracy: 0.660 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability=True)\n",
    "svm_params = {'C': [10 ** (x) for x in range(-1, 4)],\n",
    "              'kernel': ['poly', 'rbf', 'linear'],\n",
    "              'degree': [2, 3]}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(svm, svm_params, train_X, train_y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "print('Best F1 : {}'.format(best_f1))\n",
    "\n",
    "svm = SVC(C=best_params['C'], kernel=best_params['kernel'], degree=best_params['degree'], probability=True)\n",
    "metrics_svm = fit_n_times(svm, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    " ## KNN\n",
    "[Nearest Neighbors ClassificationÂ¶](https://scikit-learn.org/stable/modules/neighbors.html#classification)\n",
    "\n",
    "* Neighbors-based classification is a type of instance-based learning or non-generalizing learning: \n",
    "    * It does not construct a general internal model. \n",
    "    * Instead, it stores instances of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Classification: A point is assigned to the data class which has the most representatives within the nearest neighbours of the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* The optimal choice of the value $k$ is data-dependent: \n",
    "    * A larger $k$ suppresses the effects of noise.\n",
    "    * It makes the classification boundaries less distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Weights\n",
    "    * `weights = 'uniform'`: The value assigned to a query point is computed from a majority vote of the nearest neighbours. \n",
    "    * `weights = 'distance'`: Weights are assigned proportionally to the inverse of the distance from the query point; Nearer neighbours contribute more to the fit.\n",
    "\n",
    "ðŸ¤“: Understand the elements of `knn_params`. Vary the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'n_neighbors': 5, 'weights': 'distance'}\n",
      "F1: 0.800 | Pr: 0.670 | Re: 1.000 | AUC: 0.495 | Accuracy: 0.670 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_params = {'n_neighbors': [3, 5, 7, 9, 15, 31],\n",
    "              'weights': ['uniform', 'distance']\n",
    "              }\n",
    "\n",
    "best_params, best_f1 = run_grid_search(knn, knn_params, train_X, train_y)\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'], n_jobs=-1)\n",
    "\n",
    "metrics_knn = fit_n_times(knn, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multi-Layer Perceptron\n",
    "- A multi-layer perceptron (MLP) is a simple feedforward neural network. \n",
    "- It consists of an \n",
    "    - input layer that represents an encoding of our text input; \n",
    "    - an arbitrary number (in our case one) of hidden layers; \n",
    "    - and an output layer that represents the predicted class.\n",
    "\n",
    "<img src=\"figures/mlp.png\" alt=\"mlp\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How does a neuron work?\n",
    "\n",
    "- The terms \"dense\" and \"feedforward\" mean that all outputs of a layer are inputs for each neuron in the subsequent layer. \n",
    "- The output of a single neuron can be formulated as follows:\n",
    "    - **x** are the inputs \n",
    "    - **w** are trainable weights\n",
    "    - **y** is the output\n",
    "    - **b** is a constant bias\n",
    "    - **f<sub>A</sub>** is a non-linear activation function\n",
    "\n",
    "<img src=\"figures/neuron_formula.png\" alt=\"neuron formula\" width=\"200\"/>\n",
    "<img src=\"figures/neuron.png\" alt=\"neuron\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "- To create a non-linear model, an MLP must not consist of linear activation functions only. \n",
    "- The choice of the right activation function is a much-discussed topic in machine learning research and depends on the task to solve (regression or classification) and on whether it is applied to a hidden or the output layer. \n",
    "- These are some of the most common activation functions\n",
    "\n",
    "<img src=\"figures/activation_functions.png\" alt=\"activation functions\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Learning through back-propagation\n",
    "\n",
    "- \"Back-propagation\" is an algorithm to calculate the gradient for each weight in the neural network.\n",
    "- It estimates the contribution to the error (loss) of each weight starting from the output layer.\n",
    "- The gradient is then used by an optimization algorithm (usually stochastic gradient descent based methods) to update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 19:01:11.651802: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-28 19:01:11.759760: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 150,701\n",
      "Trainable params: 150,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 65ms/step - loss: 0.6920 - accuracy: 0.5115 - val_loss: 0.6797 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.69149, saving model to data/interim/saved_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-28 19:01:12.641249: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/interim/saved_models/assets\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6906 - accuracy: 0.5267 - val_loss: 0.6645 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.69149\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6939 - accuracy: 0.5076 - val_loss: 0.6587 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.69149\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6919 - accuracy: 0.5229 - val_loss: 0.6641 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.69149\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6882 - accuracy: 0.5229 - val_loss: 0.6721 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.69149\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6874 - accuracy: 0.5458 - val_loss: 0.6903 - val_accuracy: 0.4574\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.69149\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6855 - accuracy: 0.5420 - val_loss: 0.7032 - val_accuracy: 0.3298\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.69149\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6857 - accuracy: 0.5267 - val_loss: 0.6977 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.69149\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6856 - accuracy: 0.5305 - val_loss: 0.6855 - val_accuracy: 0.6596\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.69149\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6848 - accuracy: 0.5687 - val_loss: 0.6771 - val_accuracy: 0.6915\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.69149\n",
      "F1: 0.813 | Pr: 0.705 | Re: 0.984 | AUC: 0.570 | Accuracy: 0.702 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "simple_nn = Sequential()\n",
    "simple_nn.add(Dense(300, activation='relu', input_shape=(300,)))  # input layer\n",
    "simple_nn.add(Dropout(0.2))\n",
    "simple_nn.add(Dense(200, activation='relu')) # hidden layer\n",
    "simple_nn.add(Dropout(0.2))\n",
    "simple_nn.add(Dense(1, activation='sigmoid')) # output layer\n",
    "simple_nn.summary()\n",
    "\n",
    "simple_nn.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('data/interim/saved_models', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "history = simple_nn.fit(train_X, train_y.values,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=[checkpoint],\n",
    "                        validation_data=(test_X, test_y.values))\n",
    "y_pred_prob = simple_nn.predict(test_X)  # eliminated all .todense() --> no longer necessary when w2v instead of tfidf\n",
    "metrics_nn = print_model_metrics(test_y, y_pred_prob, return_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Methods\n",
    "Ensemble methods are machine learning techniques that combine several base models to produce one optimal predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Random Forest\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Skyler Dale](https://towardsdatascience.com/an-intuitive-explanation-of-random-forests-109b04bca343) <br>\n",
    "\n",
    "* A random forest is a meta estimator that fits several decision tree classifiers.\n",
    "* Each decision tree is trained on a different subset of the data.\n",
    "* A random subset of the full list of features is selected at every split in each decision tree. \n",
    "* The resulting trees have been exposed to different information.\n",
    "* For a regression problem, the random forest takes the average of each tree's prediction. \n",
    "* For classification, it takes the mode or majority.\n",
    "* The goal is to improve the predictive accuracy and control over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 250}\n",
      "F1: 0.807 | Pr: 0.689 | Re: 0.997 | AUC: 0.530 | Accuracy: 0.685 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "rf_params = {'n_estimators': [10, 100, 250, 500, 1000],\n",
    "             'max_depth': [None, 3, 7, 15],\n",
    "             'min_samples_split': [2, 5, 15]\n",
    "             }\n",
    "\n",
    "best_params, best_f1 = run_grid_search(rf, rf_params, train_X, train_y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                            min_samples_split=best_params['min_samples_split'],\n",
    "                            max_depth=best_params['max_depth'],\n",
    "                            n_jobs=-1)\n",
    "metrics_rf = fit_n_times(rf, train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Bagging\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [Breimann (1996)](https://link.springer.com/article/10.1007/BF00058655) <br>\n",
    "\n",
    "* Bagging stands for bootstrap and aggregation.\n",
    "* A bagging classifier fits base classifiers each on random subsets of the original dataset.\n",
    "* To form a final prediction, it aggregates their predictions (either by voting or by averaging). \n",
    "* The goal is to reduce the variance of a black-box estimator.\n",
    "* Difference to random forests:\n",
    "    * Random forests only select a subset of features at random out of the total and the best split feature from the subset is used to split each node in a tree.\n",
    "    * Bagging considers all features for splitting a node.\n",
    "\n",
    "In the following, we use SVM as a base estimator.  \n",
    "ðŸ¤“: Understand the parameters for `BaggingClassifier` and vary them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.795 | Pr: 0.670 | Re: 1.000 | AUC: 0.534 | Accuracy: 0.660 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(C=10, kernel='poly', degree=2, probability=True, verbose=0)\n",
    "\n",
    "svm_bag = BaggingClassifier(svm, n_estimators=200, \n",
    "                            max_features=0.9, \n",
    "                            max_samples=1.0, \n",
    "                            bootstrap_features=False,\n",
    "                            bootstrap=True, n_jobs=1, verbose=0)\n",
    "\n",
    "svm_bag.fit(train_X, train_y)\n",
    "y_test_prob = svm_bag.predict_proba(test_X)[:, 1]\n",
    "metrics_bag_svm = print_model_metrics(test_y, y_test_prob, return_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Stacked Generalization\n",
    "[scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization), [Wolpert (1992)](https://www.sciencedirect.com/science/article/pii/S0893608005800231)\n",
    "\n",
    "* Model stacking is a method for combining estimators to reduce their biases.\n",
    "* The predictions of each estimator are stacked together and used as input to a final estimator to compute the prediction. \n",
    "* This final estimator is trained through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "class StackingClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        lr = SGDClassifier(loss='log', alpha=0.1, penalty='elasticnet')\n",
    "        svm = SVC(C=10, kernel='poly', degree=2, probability=True)\n",
    "        nb = MultinomialNB(alpha=10000, class_prior=[0.5, 0.5])\n",
    "        knn = KNeighborsClassifier(n_neighbors=7, weights='distance', n_jobs=-1)\n",
    "        rf = RandomForestClassifier(n_estimators=50, min_samples_split=5, max_depth=15,\n",
    "                                    n_jobs=-1)  # n_estimators=250 runs for a long time\n",
    "\n",
    "        self.model_dict = dict(zip(['LR', 'SVM', 'KNN', 'RF'], [lr, svm, knn, rf]))\n",
    "        self.model_weights = {'LR': 0.9,\n",
    "                              'SVM': 0.9,\n",
    "                              #  'NB': 0.8,\n",
    "                              'KNN': 0.75,\n",
    "                              'RF': 0.75,\n",
    "                              'simple_nn': 0.7\n",
    "                              }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for model_name, model in self.model_dict.items():\n",
    "            print('Training {}'.format(model_name))\n",
    "            model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        global simple_nn\n",
    "        y_pred_prob = 0\n",
    "\n",
    "        for model_name, model in self.model_dict.items():\n",
    "            y_pred_prob += (model.predict_proba(X)[:, 1] * self.model_weights[model_name])\n",
    "\n",
    "        y_pred_prob += (simple_nn.predict(X).ravel() * self.model_weights['simple_nn'])\n",
    "        y_pred_prob /= sum(self.model_weights.values())\n",
    "        return y_pred_prob\n",
    "\n",
    "    def optimize_weights(self, X, y):\n",
    "\n",
    "        def _run_voting_clf(model_weights):\n",
    "            global simple_nn\n",
    "            y_pred_prob = 0\n",
    "            for model_name, model in self.model_dict.items():\n",
    "                y_pred_prob += (model.predict_proba(X)[:, 1] * model_weights[model_name])\n",
    "            y_pred_prob += (simple_nn.predict(X).ravel() * model_weights['simple_nn'])\n",
    "            y_pred_prob /= sum(model_weights.values())\n",
    "            f1 = print_model_metrics(y, y_pred_prob, return_metrics=True, verbose=0)[0]\n",
    "            return {'loss': -f1, 'status': STATUS_OK}\n",
    "\n",
    "        trials = Trials()\n",
    "        self.model_weights = fmin(_run_voting_clf,\n",
    "                                  space={\n",
    "                                      'LR': hp.uniform('LR', 0, 1),\n",
    "                                      'SVM': hp.uniform('SVM', 0, 1),\n",
    "                                      #    'NB': hp.uniform('NB', 0, 1),\n",
    "                                      'KNN': hp.uniform('KNN', 0, 1),\n",
    "                                      'RF': hp.uniform('RF', 0, 1),\n",
    "                                      # 'XGB': hp.uniform('XGB', 0, 1),\n",
    "                                      'simple_nn': hp.uniform('simple_nn', 0, 1),\n",
    "                                  },\n",
    "                                  algo=tpe.suggest,\n",
    "                                  max_evals=500,\n",
    "                                  trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR\n",
      "Training SVM\n",
      "Training KNN\n",
      "Training RF\n",
      "100%|â–ˆ| 500/500 [00:34<00:00, 14.62trial/s, best loss: -0.81578947\n",
      "F1: 0.816 | Pr: 0.700 | Re: 1.000 | AUC: 0.492 | Accuracy: 0.702 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stacking_clf = StackingClassifier()\n",
    "stacking_clf.fit(train_X, train_y)\n",
    "stacking_clf.optimize_weights(test_X, test_y)\n",
    "y_pred_prob = stacking_clf.predict_proba(test_X)\n",
    "np.save('data/interim/stacked_prediction.npy', y_pred_prob)  # save predictions\n",
    "metrics_stack = print_model_metrics(test_y, y_pred_prob, confusion=True, return_metrics=True)\n",
    "\n",
    "filename = 'data/interim/best_model.sav'\n",
    "pickle.dump(stacking_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>AUC</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stacking</th>\n",
       "      <td>0.816</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.813</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLR</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1   prec    rec    AUC    acc\n",
       "Stacking  0.816  0.700  1.000  0.492  0.702\n",
       "NN        0.813  0.705  0.984  0.570  0.702\n",
       "LLR       0.808  0.689  0.998  0.530  0.686\n",
       "RF        0.807  0.689  0.997  0.530  0.685\n",
       "KNN       0.800  0.670  1.000  0.495  0.670\n",
       "SVM       0.795  0.670  1.000  0.502  0.660\n",
       "Bagging   0.795  0.670  1.000  0.534  0.660"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_all = pd.DataFrame(np.stack([metrics_lr,\n",
    "                                     metrics_svm,\n",
    "                                     metrics_knn,\n",
    "                                     metrics_rf,\n",
    "                                     metrics_nn,\n",
    "                                     metrics_bag_svm,\n",
    "                                     metrics_stack\n",
    "                                     ]),\n",
    "                           columns=[\"f1\", \"prec\", \"rec\", \"AUC\", \"acc\"],\n",
    "                           index=[\"LLR\", \"SVM\", \"KNN\", \"RF\", \n",
    "                                  \"NN\", \"Bagging\", \"Stacking\"])\n",
    "metrics_all.round(decimals=3).sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# It's your turn! ðŸ§‘â€ðŸ’» ~15'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
